import argparse
import configparser
import humanfriendly
import logging
import os
import sys
from datetime import datetime

partition_command = 'gen_list'
copy_command = 'upload_sbe'
## treating arguments
# create the top-level parser
parser = argparse.ArgumentParser(prog='SnowTransferTool')
subparsers = parser.add_subparsers(dest="cmd")
# create the parser for the "gen_list" command
parser_a = subparsers.add_parser(partition_command, help=partition_command + ' --help')
parser_a.add_argument('--config_file', help='path of config file e) /tmp/config. If this argument is not present in command line, --src_dir, --filelist_dir, and --partition_size are required', action='store', default='')
parser_a.add_argument('--src_dir', help='source directory e) /data/dir1/', action='store', required='--config_file' not in sys.argv)
parser_a.add_argument('--filelist_dir', help='output destination e) /tmp/file_list/', action='store', required='--config_file' not in sys.argv)
parser_a.add_argument('--partition_size', help='size limit for each partition e) 10Tb', action='store', required='--config_file' not in sys.argv, default="10Tb")
# create the parser for the "upload_sbe" command
parser_b = subparsers.add_parser(copy_command, help=copy_command + ' --help')
parser_b.add_argument('--config_file', help='path of config file e) /tmp/config. If this argument is not present in command line, --src_dir, --bucket_name, --endpoint, and --log_dir are required', action='store', default='')
parser_b.add_argument('--src_dir', help='source directory e) /data/dir1/', action='store', required='--config_file' not in sys.argv)
parser_b.add_argument('--bucket_name', help='your bucket name e) your-bucket', action='store', required='--config_file' not in sys.argv)
parser_b.add_argument('--endpoint', help='snowball endpoint e) http://10.10.10.10:8080', action='store', required='--config_file' not in sys.argv)
parser_b.add_argument('--log_dir', help='directory that stores log files e)/tmp/JID01', action='store', required='--config_file' not in sys.argv, default='/tmp/log')
parser_b.add_argument('--profile_name', help='aws_profile_name e) sbe1', action='store', default='default')
parser_b.add_argument('--prefix_root', help='prefix root e) dir1/', action='store', default='')
parser_b.add_argument('--max_process', help='max number of thread e) 5', action='store', default=5, type=int)
parser_b.add_argument('--max_tarfile_size', help='size limit of batched files e) 1Gb', action='store', default="1Gb")
parser_b.add_argument('--compression', help='True|False compress file to "gz" format', action='store', default='False', type=bool)
parser_b.add_argument('--extract_flag', help='True|False; Set the autoextract flag', action='store', default='True', type=bool)
parser_b.add_argument('--target_file_prefix', help='prefix of the target file we are creating into the snowball', action='store', default='')

# parse argument lists
args = parser.parse_args()

if (not args.cmd):
    help_mesg = f'''usage: SnowTransferTool [-h] {{{partition_command},{copy_command}}} ...
positional arguments:
{{{partition_command},{copy_command}}}
    {partition_command}            {partition_command} --help
    {copy_command}          {copy_command} --help'''
    print(help_mesg)
    exit()

print(args)

src_dir = args.src_dir
if (args.cmd == partition_command):
    #config for gen_list
    filelist_dir = args.filelist_dir
    partition_size = humanfriendly.parse_size(args.partition_size, binary=True)

if (args.cmd == copy_command):
    #config for upload_sbe
    prefix_root = args.prefix_root ## Don't forget to add last slash '/'
    bucket_name = args.bucket_name
    profile_name = args.profile_name
    endpoint = args.endpoint
    max_process = args.max_process
    max_tarfile_size = humanfriendly.parse_size(args.max_tarfile_size, binary=True) # 10GiB, 100GiB is max limit of snowball
    compression = 'gz' if args.compression else '' # default for no compression, "gz" to enable
    target_file_prefix = args.target_file_prefix
    extract_flag = args.extract_flag # default True
    log_dir = args.log_dir

#set up logger
def setup_logger(logger_name, log_file, level=logging.INFO, sHandler=False, format='%(message)s'):
    l = logging.getLogger(logger_name)
    l.setLevel(level)
    formatter = logging.Formatter(format)
    fileHandler = logging.FileHandler(log_file, mode='a', encoding='utf-8')
    fileHandler.setFormatter(formatter)
    streamHandler = logging.StreamHandler()
    streamHandler.setFormatter(formatter)
    l.addHandler(fileHandler)
    if sHandler:
        l.addHandler(streamHandler)
    return l

def create_logger():
    current_time = datetime.now().strftime("%Y%m%d_%H%M%S")
    if (hasattr(args, 'log_dir')):
        try:
            os.makedirs(log_dir, exist_ok=True)
        except: 
            raise ValueError("Error while trying to make directory:" + log_dir)
        error_batch = log_dir + '/error-batch-%s.log' % current_time
        success_batch = log_dir + '/success-batch-%s.log' % current_time
        error_upload = log_dir + '/error-upload-%s.log' % current_time
        success_upload = log_dir + '/success-upload-%s.log' % current_time
        snow_transfer_log = log_dir + '/snowTransfer-full-%s-log' % current_time
        ## define logger
        global error_log_file, success_log_file, error_log_tar, success_log_tar, snow_transfer_full
        error_log_file = setup_logger('error_log_file', error_batch, level=log_level, sHandler=False)
        success_log_file = setup_logger('success_log_file', success_batch, level=log_level, sHandler=False)
        error_log_tar = setup_logger('error_log_tar', error_upload, level=log_level, sHandler=False)
        success_log_tar = setup_logger('success_log_tar', success_upload, level=log_level, sHandler=False)
        snow_transfer_full = setup_logger('snow_transfer_full', snow_transfer_log, level=log_level, sHandler=True, format='%(asctime)s : %(funcName)s : [%(levelname)s] : %(message)s')

log_level = logging.INFO ## DEBUG, INFO, WARNING, ERROR

def setup_config(cmd):
    list_of_globals = globals()
    config = configparser.ConfigParser()
    config.read(args.config_file)
    if (cmd == partition_command):
        if (bool(args.config_file)):
            list_of_globals['src_dir'] = config['GENLIST']['src_dir']
            list_of_globals['filelist_dir'] = config['GENLIST']['filelist_dir']
            list_of_globals['partition_size'] = humanfriendly.parse_size(config['GENLIST']['partition_size'], binary=True)
         # Check if the filelist_dir and partition_size were correctly set up
        if (not src_dir or not filelist_dir or not partition_size):
            raise ValueError("src_dir, filelist_dir and partition_size must be specified!")
        output_config = f'''Command: {cmd}
src_dir: {src_dir}
filelist_dir: {filelist_dir}
partition_size: {str(partition_size)}'''
        print(output_config)
    elif (cmd == copy_command):
        #If the config file was provided, overwrite the default setting with values in config file.
        if (bool(args.config_file)):
            list_of_globals['src_dir'] = config['UPLOAD_SBE']['src_dir']
            list_of_globals['endpoint'] = config['UPLOAD_SBE']['endpoint']
            list_of_globals['bucket_name'] = config['UPLOAD_SBE']['bucket_name']
            list_of_globals['prefix_root'] = config['UPLOAD_SBE']['prefix_root']
            list_of_globals['profile_name'] = config['UPLOAD_SBE']['profile_name']
            list_of_globals['max_process'] = int(config['UPLOAD_SBE']['max_process'])
            list_of_globals['max_tarfile_size'] = humanfriendly.parse_size(config['UPLOAD_SBE']['max_tarfile_size'], binary=True)
            list_of_globals['compression'] = 'gz' if eval(config['UPLOAD_SBE']['compression'].capitalize()) else ''
            list_of_globals['target_file_prefix'] = config['UPLOAD_SBE']['target_file_prefix']
            list_of_globals['extract_flag'] = eval(config['UPLOAD_SBE']['extract_flag'].capitalize())
            list_of_globals['log_dir'] = config['UPLOAD_SBE']['log_dir']
        if (not src_dir or not bucket_name or not endpoint or not log_dir):
            raise ValueError("src_dir, bucket_name, endpoint and log_dir must be specified!")

def human_readable_size(size, decimal_places=2):
    for unit in ['B', 'KiB', 'MiB', 'GiB', 'TiB', 'PiB']:
        if size < 1024.0 or unit == 'PiB':
            break
        size /= 1024.0
    return f"{size:.{decimal_places}f} {unit}"

def print_setting():
    output_config = f'''
command: {args.cmd}
src_dir: {src_dir}
endpoint: {endpoint}
bucket_name: {bucket_name}
log_dir: {log_dir}
profile_name: {profile_name}
prefix_root: {prefix_root}
max_process: {max_process}
max_tarfile_size: {human_readable_size(max_tarfile_size)}
compression: {bool(compression)}
target_file_prefix: {target_file_prefix}
extract_flag: {extract_flag}'''
    snow_transfer_full.info(output_config)

# start main function
if __name__ == '__main__':
    setup_config(args.cmd)
    create_logger()
    snow_transfer_full.info('Progrem started!')
    print_setting()
    snow_transfer_full.info('Program finished!')