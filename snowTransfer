import argparse
import configparser
import humanfriendly
import logging
import os
import shutil
import stat
import sys
from datetime import datetime

partition_command = 'gen_list'
copy_command = 'upload_sbe'
## treating arguments
# create the top-level parser
parser = argparse.ArgumentParser(prog='SnowTransferTool')
subparsers = parser.add_subparsers(dest="cmd")
# create the parser for the "gen_list" command
parser_a = subparsers.add_parser(partition_command, help=partition_command + ' --help')
parser_a.add_argument('--config_file', help='path of config file e) /tmp/config. If this argument is not present in command line, --src_dir, --filelist_dir, --partition_size, and --log_dir are required', action='store', default='')
parser_a.add_argument('--src_dir', help='source directory e) /data/dir1/', action='store', required='--config_file' not in sys.argv)
parser_a.add_argument('--filelist_dir', help='output destination e) /tmp/file_list/', action='store', required='--config_file' not in sys.argv)
parser_a.add_argument('--partition_size', help='size limit for each partition e) 10Tb', action='store', required='--config_file' not in sys.argv, default="10Tb")
parser_a.add_argument('--log_dir', help='directory that stores log files e)/tmp/JID01', action='store', required='--config_file' not in sys.argv, default='/tmp/snowtransfertool/log')
# create the parser for the "upload_sbe" command
parser_b = subparsers.add_parser(copy_command, help=copy_command + ' --help')
parser_b.add_argument('--config_file', help='path of config file e) /tmp/config. If this argument is not present in command line, --src_dir, --bucket_name, --endpoint, and --log_dir are required', action='store', default='')
parser_b.add_argument('--src_dir', help='source directory e) /data/dir1/', action='store', required='--config_file' not in sys.argv)
parser_b.add_argument('--bucket_name', help='your bucket name e) your-bucket', action='store', required='--config_file' not in sys.argv)
parser_b.add_argument('--endpoint', help='snowball endpoint e) http://10.10.10.10:8080', action='store', required='--config_file' not in sys.argv)
parser_b.add_argument('--log_dir', help='directory that stores log files e)/tmp/JID01', action='store', required='--config_file' not in sys.argv, default='/tmp/snowtransfertool/log')
parser_b.add_argument('--profile_name', help='aws_profile_name e) sbe1', action='store', default='default')
parser_b.add_argument('--prefix_root', help='prefix root e) dir1/', action='store', default='')
parser_b.add_argument('--max_process', help='max number of thread e) 5', action='store', default=5, type=int)
parser_b.add_argument('--max_tarfile_size', help='size limit of batched files e) 1Gb', action='store', default="1Gb")
parser_b.add_argument('--compression', help='True|False compress file to "gz" format', action='store', default='False', type=bool)
parser_b.add_argument('--extract_flag', help='True|False; Set the autoextract flag', action='store', default='True', type=bool)
parser_b.add_argument('--target_file_prefix', help='prefix of the target file we are creating into the snowball', action='store', default='')

# parse argument lists
args = parser.parse_args()

if (not args.cmd):
    help_mesg = f'''usage: SnowTransferTool [-h] {{{partition_command},{copy_command}}} ...
positional arguments:
{{{partition_command},{copy_command}}}
    {partition_command}            {partition_command} --help
    {copy_command}          {copy_command} --help'''
    print(help_mesg)
    exit()

print(args)

# src and log dir are required in both commands.
src_dir = args.src_dir
log_dir = args.log_dir
if (args.cmd == partition_command):
    #config for gen_list
    filelist_dir = args.filelist_dir
    partition_size = humanfriendly.parse_size(args.partition_size, binary=True)

if (args.cmd == copy_command):
    #config for upload_sbe
    prefix_root = args.prefix_root ## Don't forget to add last slash '/'
    bucket_name = args.bucket_name
    profile_name = args.profile_name
    endpoint = args.endpoint
    max_process = args.max_process
    max_tarfile_size = humanfriendly.parse_size(args.max_tarfile_size, binary=True) # 10GiB, 100GiB is max limit of snowball
    compression = 'gz' if args.compression else '' # default for no compression, "gz" to enable
    target_file_prefix = args.target_file_prefix
    extract_flag = args.extract_flag # default True

#set up logger
def setup_logger(logger_name, log_file, level=logging.INFO, sHandler=False, format='%(message)s'):
    l = logging.getLogger(logger_name)
    l.setLevel(level)
    formatter = logging.Formatter(format)
    fileHandler = logging.FileHandler(log_file, mode='a', encoding='utf-8')
    fileHandler.setFormatter(formatter)
    streamHandler = logging.StreamHandler()
    streamHandler.setFormatter(formatter)
    l.addHandler(fileHandler)
    if sHandler:
        l.addHandler(streamHandler)
    return l

def create_logger():
    current_time = datetime.now().strftime("%Y%m%d_%H%M%S")
    if (hasattr(args, 'log_dir')):
        try:
            os.makedirs(log_dir, exist_ok=True)
        except: 
            raise ValueError("Error while trying to make directory:" + log_dir)
        error_batch = log_dir + '/error-batch-%s.log' % current_time
        success_batch = log_dir + '/success-batch-%s.log' % current_time
        error_upload = log_dir + '/error-upload-%s.log' % current_time
        success_upload = log_dir + '/success-upload-%s.log' % current_time
        snow_transfer_log = log_dir + '/snowTransfer-full-%s-log' % current_time
        ## define logger
        global error_log_file, success_log_file, error_log_tar, success_log_tar, snow_transfer_full
        snow_transfer_full = setup_logger('snow_transfer_full', snow_transfer_log, level=log_level, sHandler=True, format='%(asctime)s : %(funcName)s : [%(levelname)s] : %(message)s')
        if (args.cmd == copy_command):
            error_log_file = setup_logger('error_log_file', error_batch, level=log_level, sHandler=False)
            success_log_file = setup_logger('success_log_file', success_batch, level=log_level, sHandler=False)
            error_log_tar = setup_logger('error_log_tar', error_upload, level=log_level, sHandler=False)
            success_log_tar = setup_logger('success_log_tar', success_upload, level=log_level, sHandler=False)

log_level = logging.INFO ## DEBUG, INFO, WARNING, ERROR

def setup_config(cmd):
    list_of_globals = globals()
    config = configparser.ConfigParser()
    config.read(args.config_file)
    if (cmd == partition_command):
        if (bool(args.config_file)):
            list_of_globals['src_dir'] = config['GENLIST']['src_dir']
            list_of_globals['filelist_dir'] = config['GENLIST']['filelist_dir']
            list_of_globals['partition_size'] = humanfriendly.parse_size(config['GENLIST']['partition_size'], binary=True)
            list_of_globals['log_dir'] = config['GENLIST']['log_dir']
         # Check if the filelist_dir and partition_size were correctly set up
        if (not src_dir or not filelist_dir or not partition_size):
            raise ValueError("src_dir, filelist_dir and partition_size must be specified!")
        output_config = f'''Command: {cmd}
src_dir: {src_dir}
filelist_dir: {filelist_dir}
partition_size: {str(partition_size)}'''
        print(output_config)
    elif (cmd == copy_command):
        #If the config file was provided, overwrite the default setting with values in config file.
        if (bool(args.config_file)):
            list_of_globals['src_dir'] = config['UPLOAD_SBE']['src_dir']
            list_of_globals['endpoint'] = config['UPLOAD_SBE']['endpoint']
            list_of_globals['bucket_name'] = config['UPLOAD_SBE']['bucket_name']
            list_of_globals['prefix_root'] = config['UPLOAD_SBE']['prefix_root']
            list_of_globals['profile_name'] = config['UPLOAD_SBE']['profile_name']
            list_of_globals['max_process'] = int(config['UPLOAD_SBE']['max_process'])
            list_of_globals['max_tarfile_size'] = humanfriendly.parse_size(config['UPLOAD_SBE']['max_tarfile_size'], binary=True)
            list_of_globals['compression'] = 'gz' if eval(config['UPLOAD_SBE']['compression'].capitalize()) else ''
            list_of_globals['target_file_prefix'] = config['UPLOAD_SBE']['target_file_prefix']
            list_of_globals['extract_flag'] = eval(config['UPLOAD_SBE']['extract_flag'].capitalize())
            list_of_globals['log_dir'] = config['UPLOAD_SBE']['log_dir']
        if (not src_dir or not bucket_name or not endpoint or not log_dir):
            raise ValueError("src_dir, bucket_name, endpoint and log_dir must be specified!")

def human_readable_size(size, decimal_places=2):
    for unit in ['B', 'KiB', 'MiB', 'GiB', 'TiB', 'PiB']:
        if size < 1024.0 or unit == 'PiB':
            break
        size /= 1024.0
    return f"{size:.{decimal_places}f} {unit}"

def print_setting():
    if (args.cmd == partition_command):
        output_config = f'''
Command: {args.cmd}
src_dir: {src_dir}
filelist_dir: {filelist_dir}
partition_size: {human_readable_size(partition_size)}
log_dir: {log_dir}'''
        snow_transfer_full.info(output_config)
    elif (args.cmd == copy_command):
        output_config = f'''
command: {args.cmd}
src_dir: {src_dir}
endpoint: {endpoint}
bucket_name: {bucket_name}
log_dir: {log_dir}
profile_name: {profile_name}
prefix_root: {prefix_root}
max_process: {max_process}
max_tarfile_size: {human_readable_size(max_tarfile_size)}
compression: {bool(compression)}
target_file_prefix: {target_file_prefix}
extract_flag: {extract_flag}'''
        snow_transfer_full.info(output_config)

def write_to_file(dict):
    for key, value in dict.items():
        with open(key, 'a') as f:
            for line in value:
                f.write("%s\n" %line)
    return 0

#Error handler for ``shutil.rmtree``
def handle_rmtree_err(func, path, exc_info):
    snow_transfer_full.error("Error while trying to clean: %s\n%s" % (path, exc_info))
    exit()

def gen_filelist(src):
    sum_size = 0
    fl_prefix = 'fl_'
    fl_index = 1
    delimiter = ', '
    num_file_processed = 0
    file_target_mp = dict()
    customer_input = input ("This operation will clean all the contents in: %s. Continue? Y/N\n" % filelist_dir)
    if (customer_input[0].lower() != 'y'):
        exit()
    shutil.rmtree(filelist_dir, ignore_errors=False, onerror=handle_rmtree_err)
    try:
        os.makedirs(filelist_dir)
    except: 
        raise ValueError("Error while trying to make directory:" + filelist_dir)
    snow_transfer_full.info('generating file list by size %s bytes' % partition_size)
    for r,d,f in os.walk(src):
        fl_name = filelist_dir + '/' + fl_prefix + str(fl_index) + ".txt"
        for file in f:
            file_name = os.path.join(r,file)
            try:
                f_meta = os.stat(file_name)
            except Exception as e:
                if(os.path.islink(file_name)):
                    # continue if it is a symlink
                    continue
                else:
                    snow_transfer_full.error(str(e))
            f_size = f_meta.st_size
            sum_size = sum_size + f_size
            f_info = [file_name ,str(f_size)]
            f_info_str = delimiter.join(f_info)
            if partition_size < sum_size:
                if (sum_size - f_size != 0):
                    fl_index += 1
                    fl_name = filelist_dir + '/' + fl_prefix + str(fl_index) + ".txt"
                snow_transfer_full.info('Part #%d: size = %d' % (fl_index - 1, sum_size - f_size))
                sum_size = f_size
            if fl_name not in file_target_mp:
                file_target_mp[fl_name] = []
            file_target_mp[fl_name].append(f_info_str)
            num_file_processed += 1
            # flush the dict every 100000 file to avoid OOM
            if (num_file_processed % 100000 == 0):
                snow_transfer_full.info('Number of scanned file: %d', num_file_processed)
                write_to_file(file_target_mp)
                file_target_mp = dict()
    write_to_file(file_target_mp)
    fl_name = filelist_dir + '/' + fl_prefix + str(fl_index) + ".txt"
    snow_transfer_full.info('Part #%d: size = %d' % (fl_index, sum_size))
    snow_transfer_full.info('Number of scanned file: %d, symlink and empty folders were ignored', num_file_processed)
    snow_transfer_full.info('File lists are generated!!')
    snow_transfer_full.info('Check %s' % filelist_dir)
    return 0

# start main function
if __name__ == '__main__':
    setup_config(args.cmd)
    create_logger()
    snow_transfer_full.info('Program started!')
    print_setting()
    #valid_setting()
    if args.cmd == copy_command:
        pass
    elif args.cmd == partition_command:
        gen_filelist(src_dir)
    snow_transfer_full.info('Program finished!')